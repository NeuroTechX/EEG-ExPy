{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# P300 Decoding\n\nThis example runs a set of machine learning algorithms on the P300 cats/dogs\ndataset, and compares them in terms of classification performance. \n\nThe data used is exactly the same as in the P300 `load_and_visualize` example. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some standard pythonic imports\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os,numpy as np,pandas as pd\nfrom collections import OrderedDict\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# MNE functions\nfrom mne import Epochs,find_events\nfrom mne.decoding import Vectorizer\n\n# EEG-Notebooks functions\nfrom eegnb.analysis.analysis_utils import load_data\nfrom eegnb.datasets import fetch_dataset\n\n# Scikit-learn and Pyriemann ML functionalities\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\nfrom pyriemann.estimation import ERPCovariances, XdawnCovariances, Xdawn\nfrom pyriemann.tangentspace import TangentSpace\nfrom pyriemann.classification import MDM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n\n( See the P300 `load_and_visualize` example for further description of this)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eegnb_data_path = os.path.join(os.path.expanduser('~/'),'.eegnb', 'data')    \np300_data_path = os.path.join(eegnb_data_path, 'visual-P300', 'eegnb_examples')\n\n# If dataset hasn't been downloaded yet, download it \nif not os.path.isdir(p300_data_path):\n    fetch_dataset(data_dir=eegnb_data_path, experiment='visual-P300', site='eegnb_examples')        \n\n\nsubject = 1\nsession = 1\nraw = load_data(subject,session,\n                experiment='visual-P300', site='eegnb_examples', device_name='muse2016',\n                data_dir = eegnb_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filteriing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.filter(1,30, method='iir')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Epoching\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create an array containing the timestamps and type of each stimulus (i.e. face or house)\nevents = find_events(raw)\nevent_id = {'Non-Target': 1, 'Target': 2}\nepochs = Epochs(raw, events=events, event_id=event_id,\n                tmin=-0.1, tmax=0.8, baseline=None,                                                                                     reject={'eeg': 100e-6}, preload=True,                                                                                   verbose=False, picks=[0,1,2,3])\n\nprint('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n\nepochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classfication\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clfs = OrderedDict()\nclfs['Vect + LR'] = make_pipeline(Vectorizer(), StandardScaler(), LogisticRegression())\nclfs['Vect + RegLDA'] = make_pipeline(Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\nclfs['Xdawn + RegLDA'] = make_pipeline(Xdawn(2, classes=[1]), Vectorizer(), LDA(shrinkage='auto', solver='eigen'))\n\nclfs['XdawnCov + TS'] = make_pipeline(XdawnCovariances(estimator='oas'), TangentSpace(), LogisticRegression())\nclfs['XdawnCov + MDM'] = make_pipeline(XdawnCovariances(estimator='oas'), MDM())\n\n\nclfs['ERPCov + TS'] = make_pipeline(ERPCovariances(), TangentSpace(), LogisticRegression())\nclfs['ERPCov + MDM'] = make_pipeline(ERPCovariances(), MDM())\n\n# format data\nepochs.pick_types(eeg=True)\nX = epochs.get_data() * 1e6\ntimes = epochs.times\ny = epochs.events[:, -1]\n\n# define cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.25, random_state=42)\n\n# run cross validation for each pipeline\nauc = []\nmethods = []\nfor m in clfs:\n    res = cross_val_score(clfs[m], X, y==2, scoring='roc_auc', cv=cv, n_jobs=-1)\n    auc.extend(res)\n    methods.extend([m]*len(res))\n    \nresults = pd.DataFrame(data=auc, columns=['AUC'])\nresults['Method'] = methods\n\nplt.figure(figsize=[8,4])\nsns.barplot(data=results, x='AUC', y='Method')\nplt.xlim(0.2, 0.85)\nsns.despine()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}