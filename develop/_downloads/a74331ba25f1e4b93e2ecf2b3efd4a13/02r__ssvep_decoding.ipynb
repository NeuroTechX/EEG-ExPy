{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SSVEP Decoding\n\nThis notebook runs only the data analysis part of experiment.\n\nLook at the notes to see how this can be run on the web with binder or google collab.\n\nAll of the additional notes are removed; only the code cells are kept.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Some standard pythonic imports\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os,numpy as np,pandas as pd\nfrom collections import OrderedDict\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# MNE functions\nfrom mne import Epochs,find_events\nfrom mne.decoding import Vectorizer\n\n# EEG-Notebooks functions\nfrom eegnb.analysis.analysis_utils import load_data\nfrom eegnb.datasets import fetch_dataset\n\n# Scikit-learn and Pyriemann ML functionalities\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import cross_val_score, StratifiedShuffleSplit\nfrom pyriemann.estimation import Covariances, ERPCovariances, XdawnCovariances\nfrom pyriemann.spatialfilters import CSP\nfrom pyriemann.tangentspace import TangentSpace\nfrom pyriemann.classification import MDM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n\n( See the ssvep `load_and_visualize` example for further description of this)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eegnb_data_path = os.path.join(os.path.expanduser('~/'),'.eegnb', 'data')    \nssvep_data_path = os.path.join(eegnb_data_path, 'visual-SSVEP', 'eegnb_examples')\n\n# If dataset hasn't been downloaded yet, download it \nif not os.path.isdir(ssvep_data_path):\n    fetch_dataset(data_dir=eegnb_data_path, experiment='visual-SSVEP', site='eegnb_examples')        \n\nsubject = 1\nsession = 1\nraw = load_data(subject, session, \n                experiment='visual-SSVEP', site='eegnb_examples', device_name='muse2016',\n                data_dir = eegnb_data_path,\n                replace_ch_names={'Right AUX': 'POz'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Epoching\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we will chunk (epoch) the data into segments representing the data 100ms before to 800ms after each stimulus.\n# Note: we will not reject epochs here because the amplitude of the SSVEP at POz is so large it is difficult to separate from eye blinks\n\nevents = find_events(raw)\nevent_id = {'30 Hz': 1, '20 Hz': 2}\nepochs = Epochs(raw, events=events, event_id=event_id, \n                tmin=-0.5, tmax=4, baseline=None, preload=True,\n                verbose=False, picks=[0, 1, 2, 3, 4])\nprint('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can use a filter bank approach on the original 4 Muse electrodes (to see how the headband alone without external electrodes could be used to classify SSVEP):\n\n#    - Apply bandpass filters around both stimulation frequencies\n#    - Concatenate bandpass-filtered channels\n#    - Extract epochs (from 1 to 3 s after stimulus onset, to avoid classifying the ERP)\n#    - Apply common classification pipelines\n\n# Bandpass filter the raw data\nmuse_raw = raw.drop_channels(['POz'])\nraw_filt_30Hz = muse_raw.copy().filter(25, 35, method='iir')\nraw_filt_20Hz = muse_raw.copy().filter(15, 25, method='iir')\nraw_filt_30Hz.rename_channels(lambda x: x + '_30Hz')\nraw_filt_20Hz.rename_channels(lambda x: x + '_20Hz')\n\n# Concatenate with the bandpass filtered channels\nraw_all = raw_filt_30Hz.add_channels([raw_filt_20Hz], \n                                            force_update_info=True)\n\n# Extract epochs\nevents = find_events(raw_all)\nevent_id = {'30 Hz': 1, '20 Hz': 2}\n\nepochs_all = Epochs(raw_all, events=events, event_id=event_id, \n                    tmin=1, tmax=3, baseline=None, \n                    reject={'eeg': 100e-6}, preload=True, verbose=False,)\n\nepochs_all.pick_types(eeg=True)\nX = epochs_all.get_data() * 1e6\ntimes = epochs.times\ny = epochs_all.events[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Next, we will use 4 different machine learning pipelines to classify the SSVEP based on the data we collected. The\n\n# - CSP + RegLDA : Common Spatial Patterns + Regularized Linear Discriminat Analysis. This is a very common EEG analysis pipeline.\n# - Cov + TS : Covariance + Tangent space mapping. One of the most reliable Riemannian geometry-based pipelines.\n# - Cov + MDM: Covariance + MDM. A very simple, yet effective (for low channel count), Riemannian geometry classifier.\n# - CSP + Cov + TS: Common Spatial Patterns + Covariance + Tangent spacem mapping. Riemannian pipeline with the standard CSP procedure beforehand\n\n# Evaluation is done through cross-validation, with area-under-the-curve (AUC) as metric (AUC is probably the best metric for binary and unbalanced classification problem)\n\n# Note: because we're doing machine learning here, the following cell may take a while to complete\n\nclfs = OrderedDict()\nclfs['CSP + RegLDA'] = make_pipeline(Covariances(), CSP(4), LDA(shrinkage='auto', solver='eigen'))\nclfs['Cov + TS'] = make_pipeline(Covariances(), TangentSpace(), LogisticRegression())\nclfs['Cov + MDM'] = make_pipeline(Covariances(), MDM())\nclfs['CSP + Cov + TS'] = make_pipeline(Covariances(), CSP(4, log=False), TangentSpace(), LogisticRegression())\n\n# define cross validation \ncv = StratifiedShuffleSplit(n_splits=20, test_size=0.25, \n                                        random_state=42)\n\n# run cross validation for each pipeline\nauc = []\nmethods = []\nfor m in clfs:\n    print(m)     \n    try:    \n        res = cross_val_score(clfs[m], X, y==2, scoring='roc_auc',cv=cv, n_jobs=-1)\n        auc.extend(res)\n        methods.extend([m]*len(res))\n    except:\n        pass\n    \nresults = pd.DataFrame(data=auc, columns=['AUC'])\nresults['Method'] = methods\n\nfig = plt.figure(figsize=[8,4])\nsns.barplot(data=results, x='AUC', y='Method')\nplt.xlim(0.4, 1)\nsns.despine()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}