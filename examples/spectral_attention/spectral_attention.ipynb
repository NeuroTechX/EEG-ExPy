{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa0b7fd1-8785-422f-9341-e723143218c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc00d08-d745-4233-b14e-98977ff86ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: nbagg. Using qt instead.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook #if using qt, do not use this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1863df1-8079-49ff-9446-255b617fcd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeya\\EEG-ExPy\\venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "print(sys.executable)  # Should point to your virtual environment's Python\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634f1e18-08af-4e05-9f95-f528c99b0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eegnb.datasets.datasets import fetch_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684aaff2-6535-4796-b7e4-46c33d78fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Navigate up from the notebook's directory to the project root\n",
    "project_root = Path(os.getcwd()).resolve().parent.parent  # Up to `EEG-ExPy`\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import gdown  # Should now work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c24ca4-3325-4c3b-85df-844f3b486dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1oStfxzEqf36R5d-2Auyw4DLnPj9E_FAH\n",
      "From (redirected): https://drive.google.com/uc?id=1oStfxzEqf36R5d-2Auyw4DLnPj9E_FAH&confirm=t&uuid=d56dcd58-994c-45a9-a7ba-7d624cb9adca\n",
      "To: C:\\Users\\reeya\\EEG-ExPy\\examples\\data\\downloaded_data.zip\n",
      "100%|████████████████████████████| 33.1M/33.1M [00:00<00:00, 52.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'  # or anywhere you'd like the data to download to\n",
    "experiment = 'visual-N170'\n",
    "subject_list = [1]  # or 'all' for everything\n",
    "\n",
    "# Download and get list of .csv files\n",
    "file_paths = fetch_dataset(data_dir=data_dir, experiment=experiment, subjects=subject_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c095a52a-b1bc-4f12-a1d4-3d133877461f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(file_paths)} files:\")\n",
    "print(file_paths[:3])  # Print the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d369a1c1-694a-4847-9168-27d86da6c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "file_paths = fetch_dataset(\n",
    "    data_dir='../data', \n",
    "    experiment='visual-N170',\n",
    "    site='eegnb_examples',\n",
    "    device='muse2016',\n",
    "    subjects=[1],\n",
    "    sessions=[1]\n",
    ")\n",
    "print(file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2bfc463-e02a-4f20-91cb-7bebafc0705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.30.01.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.32.50.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.35.26.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.40.17.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.42.33.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0001\\\\session001\\\\data_2017-09-13-15.45.08.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0002\\\\session001\\\\data_2018-04-15-21.18.48.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0002\\\\session001\\\\data_2018-04-15-21.21.20.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session001\\\\data_2018-05-14-19.14.37.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session001\\\\data_2018-05-14-19.38.10.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session002\\\\data_2018-05-14-19.35.55.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session002\\\\data_2018-05-14-19.38.10.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session003\\\\recording_2018-05-29-20.14.04.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0003\\\\session003\\\\recording_2018-05-31-16.03.39.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0010\\\\session001\\\\recording_2018-06-06-14.34.14.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0010\\\\session001\\\\recording_2018-06-06-15.06.45.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0010\\\\session002\\\\recording_2018-06-06-14.42.27.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0010\\\\session003\\\\recording_2018-06-06-14.45.33.csv', '../data\\\\visual-N170\\\\eegnb_examples\\\\muse2016\\\\subject0011\\\\session001\\\\recording_2018-06-06-18.42.19.csv']\n"
     ]
    }
   ],
   "source": [
    "file_paths = fetch_dataset(\n",
    "    data_dir='../data', \n",
    "    experiment='visual-N170',\n",
    "    subjects='all',\n",
    "    sessions='all'\n",
    ")\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a699bd29-236d-45a1-8c1e-e21c0ab79504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637d5555-a1c8-45d6-89e9-23a885afabda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.30.01.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.32.50.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.35.26.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.40.17.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.42.33.csv with shape (30720, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0001\\session001\\data_2017-09-13-15.45.08.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0002\\session001\\data_2018-04-15-21.18.48.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0002\\session001\\data_2018-04-15-21.21.20.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session001\\data_2018-05-14-19.14.37.csv with shape (30720, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session001\\data_2018-05-14-19.38.10.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session002\\data_2018-05-14-19.35.55.csv with shape (30720, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session002\\data_2018-05-14-19.38.10.csv with shape (30732, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session003\\recording_2018-05-29-20.14.04.csv with shape (30564, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0003\\session003\\recording_2018-05-31-16.03.39.csv with shape (30552, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0010\\session001\\recording_2018-06-06-14.34.14.csv with shape (30552, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0010\\session001\\recording_2018-06-06-15.06.45.csv with shape (6180, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0010\\session002\\recording_2018-06-06-14.42.27.csv with shape (30552, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0010\\session003\\recording_2018-06-06-14.45.33.csv with shape (30564, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n",
      "Loaded ../data\\visual-N170\\eegnb_examples\\muse2016\\subject0011\\session001\\recording_2018-06-06-18.42.19.csv with shape (30564, 7) and columns: ['timestamps', 'TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']\n"
     ]
    }
   ],
   "source": [
    "# Load CSVs into pandas\n",
    "eeg_dataframes = []\n",
    "for path in file_paths:\n",
    "    df = pd.read_csv(path)\n",
    "    eeg_dataframes.append(df)\n",
    "    print(f\"Loaded {path} with shape {df.shape} and columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca639aad-0bd2-4e07-aa20-0545bf97dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 19 EEG recordings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "channel_names = ['TP9', 'AF7', 'AF8', 'TP10']\n",
    "parsed_data = []\n",
    "\n",
    "for df in eeg_dataframes:\n",
    "    signals = df[channel_names].to_numpy().T  # Shape: (n_channels, n_samples)\n",
    "    timestamps = df['timestamps'].to_numpy()\n",
    "    \n",
    "    # Extract event markers (nonzero entries)\n",
    "    marker_series = df['Marker0'].fillna(0)\n",
    "    events = np.where(marker_series != 0)[0]  # Indices where stimulus was shown\n",
    "    \n",
    "    parsed_data.append({\n",
    "        'signals': signals,\n",
    "        'timestamps': timestamps,\n",
    "        'events': events,\n",
    "    })\n",
    "\n",
    "print(f\"Parsed {len(parsed_data)} EEG recordings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc5e82e-de2e-432b-80c2-199618a5c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals shape: (4, 30732)\n",
      "Event indices: [   70   198   381   520   683   817   975  1142  1311  1457  1618  1789\n",
      "  1967  2102  2272  2426  2591  2725  2870  3006  3181  3355  3529  3686\n",
      "  3864  4002  4151  4330  4467  4610  4755  4908  5045  5216  5368  5498\n",
      "  5633  5779  5958  6123  6264  6418  6594  6744  6875  7050  7213  7357\n",
      "  7529  7667  7824  7974  8111  8286  8458  8635  8789  8952  9095  9253\n",
      "  9383  9540  9722  9884 10017 10190 10360 10510 10644 10809 10954 11089\n",
      " 11256 11414 11548 11717 11883 12023 12190 12315 12472 12603 12731 12897\n",
      " 13030 13172 13320 13495 13636 13803 13936 14081 14239 14389 14554 14731\n",
      " 14904 15060 15195 15326 15494 15644 15787 15920 16077 16231 16410 16579\n",
      " 16729 16861 17030 17176 17308 17473 17609 17788 17949 18110 18282 18434\n",
      " 18600 18755 18911 19091 19248 19408 19554 19721 19875 20043 20210 20342\n",
      " 20473 20650 20799 20976 21108 21247 21413 21580 21727 21869 22018 22188\n",
      " 22337 22507 22635 22765 22914 23066 23209 23341 23483 23623 23797 23975\n",
      " 24112 24280 24416 24570 24731 24899 25058 25218 25376 25508 25643 25809\n",
      " 25962 26101 26247 26419 26562 26702 26862 27023 27175 27327 27487 27628\n",
      " 27762 27912 28041 28195 28343 28476 28620 28792 28967 29123 29279 29429\n",
      " 29609 29781 29939 30103 30261]\n"
     ]
    }
   ],
   "source": [
    "example = parsed_data[0]\n",
    "print(\"Signals shape:\", example['signals'].shape)\n",
    "print(\"Event indices:\", example['events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7f7cb30-8bfb-4b6f-89f0-1fb8934062a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=4, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Effective window size : 8.000 (s)\n",
      "Plotting power spectral density (dB=True).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<MNELineFigure size 1000x350 with 1 Axes>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "# Step 1: Define channel info\n",
    "sfreq = 256  # Hz, Muse standard sampling rate\n",
    "ch_names = ['TP9', 'AF7', 'AF8', 'TP10']\n",
    "ch_types = ['eeg'] * 4\n",
    "\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "# Step 2: Create RawArray from first example\n",
    "signals = parsed_data[0]['signals']  # Shape: (4, n_samples)\n",
    "raw = mne.io.RawArray(signals, info)\n",
    "\n",
    "# Step 3: Add montage (optional but good for head map visuals)\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "raw.set_montage(montage, match_case=False)\n",
    "\n",
    "# Step 4: Plot time series and PSD\n",
    "raw.plot(n_channels=4, duration=10, scalings='auto', title='Raw EEG (Muse)')\n",
    "\n",
    "# Compute PSD with frequency limits directly\n",
    "psd = raw.compute_psd(fmin=1, fmax=50)\n",
    "\n",
    "# Plot with average over channels\n",
    "psd.plot(average=True, picks='eeg', show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8afdea0e-653b-4a99-9b10-84bad0d34588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.004 (s)\n"
     ]
    }
   ],
   "source": [
    "# Compute PSD from epochs using new API\n",
    "psd = epochs.compute_psd(method='welch', fmin=1, fmax=40)\n",
    "\n",
    "# Get data and frequencies\n",
    "psds = psd.get_data()        # shape: (n_epochs, n_channels, n_freqs)\n",
    "freqs = psd.freqs            # shape: (n_freqs,)\n",
    "\n",
    "\n",
    "# Take log-power (better for ML models)\n",
    "X = np.log(psds)  # shape: (n_epochs, n_channels, n_freqs)\n",
    "X = X.reshape(X.shape[0], -1)  # Flatten to (samples, features)\n",
    "\n",
    "# Create labels (all events are '1' for now)\n",
    "y = np.ones(X.shape[0])  # You can later map different event types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ac69a48-e94a-4704-b160-357a5c9e527f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "197 matching events found\n",
      "Setting baseline interval to [-0.19921875, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 197 events and 257 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "from mne import Epochs\n",
    "\n",
    "# Assuming parsed_data[0] is your sample subject/session\n",
    "event_samples = parsed_data[0]['events']  # Sample indices where stimulus happened\n",
    "\n",
    "# MNE expects events as shape (n_events, 3): [sample_index, 0, event_id]\n",
    "events = np.array([[idx, 0, 1] for idx in event_samples])  # Using 1 as the dummy event label\n",
    "\n",
    "# Now create epochs\n",
    "epochs = Epochs(\n",
    "    raw_filtered,\n",
    "    events=events,\n",
    "    event_id={'stimulus': 1},\n",
    "    tmin=-0.2,  # 200 ms before event\n",
    "    tmax=0.8,   # 800 ms after event\n",
    "    baseline=(None, 0),\n",
    "    preload=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba16d635-5baa-4b0b-aa81-ce390cf751b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=4, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 40 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 40.00 Hz\n",
      "- Upper transition bandwidth: 10.00 Hz (-6 dB cutoff frequency: 45.00 Hz)\n",
      "- Filter length: 845 samples (3.301 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "none\n",
      "Channels marked as bad:\n",
      "none\n",
      "Channels marked as bad:\n",
      "none\n",
      "Channels marked as bad:\n",
      "none\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "# Step 1: Setup info again (if needed)\n",
    "sfreq = 256  # Muse headset sampling rate\n",
    "ch_names = ['TP9', 'AF7', 'AF8', 'TP10']\n",
    "ch_types = ['eeg'] * 4\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "\n",
    "# Step 2: Get signals from parsed_data (transpose to shape: channels × samples)\n",
    "signals = parsed_data[0]['signals']  # Should be shape (4, N)\n",
    "raw = mne.io.RawArray(signals, info)\n",
    "\n",
    "# Step 3: Optional: add montage for plotting\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "raw.set_montage(montage, match_case=False)\n",
    "\n",
    "# Step 4: Filter the raw EEG\n",
    "raw_filtered = raw.copy().filter(l_freq=1.0, h_freq=40.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cef5223-c17c-4425-88a2-5523a91cad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['signals', 'timestamps', 'events'])\n"
     ]
    }
   ],
   "source": [
    "print(parsed_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33c10737-1e3e-4763-aab5-e95278665299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected event types and counts:\n",
      "Event 1: 89 occurrences\n",
      "Event 2: 103 occurrences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: assuming 'df' is your loaded CSV DataFrame\n",
    "marker_col = df['Marker0'].values  # numpy array of Marker0 column\n",
    "\n",
    "# Find indices where Marker0 is nonzero (assuming 0 means no event)\n",
    "event_indices = np.where(marker_col != 0)[0]\n",
    "\n",
    "# Extract corresponding event labels (nonzero values)\n",
    "event_labels = marker_col[event_indices].astype(int)\n",
    "\n",
    "# Create MNE-style events array: [sample_index, 0, event_label]\n",
    "events = np.column_stack((event_indices, np.zeros_like(event_indices), event_labels))\n",
    "\n",
    "print(\"Detected event types and counts:\")\n",
    "unique, counts = np.unique(event_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Event {u}: {c} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "309c2b7a-e78d-45e1-bdc5-39fadc61e8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "192 matching events found\n",
      "Setting baseline interval to [-0.19921875, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 192 events and 257 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "event_id = {str(k): k for k in np.unique(event_labels)}  # e.g. {'1': 1, '2': 2}\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw_filtered,\n",
    "    events=events,\n",
    "    event_id=event_id,\n",
    "    tmin=-0.2,\n",
    "    tmax=0.8,\n",
    "    baseline=(None, 0),\n",
    "    preload=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2172e19-6cb8-428f-addc-ee0ee2123e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.004 (s)\n"
     ]
    }
   ],
   "source": [
    "psd = epochs.compute_psd(method='welch', fmin=1, fmax=40)\n",
    "X = np.log(psd.get_data())\n",
    "X = X.reshape(X.shape[0], -1)  # flatten for ML\n",
    "y = epochs.events[:, 2]        # true labels: 1 or 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00e50ebc-0795-4502-90d3-e6cc1a20cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy: 0.55 ± 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(f\"Cross-validated accuracy: {scores.mean():.2f} ± {scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fe9c00e-08e6-4b8a-8420-89369ffcf917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 CSV files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_folder = '../data/visual-N170/eegnb_examples/muse2016/'\n",
    "\n",
    "all_files = []\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            all_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(all_files)} CSV files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8cba37e4-2588-478a-9951-59a5489bc224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.30.01.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.32.50.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.35.26.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.40.17.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.42.33.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0001\\session001\\data_2017-09-13-15.45.08.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0002\\session001\\data_2018-04-15-21.18.48.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0002\\session001\\data_2018-04-15-21.21.20.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session001\\data_2018-05-14-19.14.37.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session001\\data_2018-05-14-19.38.10.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session002\\data_2018-05-14-19.35.55.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session002\\data_2018-05-14-19.38.10.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session003\\recording_2018-05-29-20.14.04.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30564\n",
      "    Range : 0 ... 30563 =      0.000 ...   119.387 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0003\\session003\\recording_2018-05-31-16.03.39.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30552\n",
      "    Range : 0 ... 30551 =      0.000 ...   119.340 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Processing ../data/visual-N170/eegnb_examples/muse2016/subject0010\\session001\\recording_2018-06-06-14.34.14.csv ...\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30552\n",
      "    Range : 0 ... 30551 =      0.000 ...   119.340 secs\n",
      "Ready.\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeya\\AppData\\Local\\Temp\\ipykernel_512\\2851822936.py:45: RuntimeWarning: All epochs were dropped!\n",
      "You might need to alter reject/flat-criteria or drop bad channels to avoid this. You can use Epochs.plot_drop_log() to see which channels are responsible for the dropping of epochs.\n",
      "  epochs = mne.Epochs(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "epochs._get_data() can't run because this Epochs-object is empty. You might want to check Epochs.drop_log or Epochs.plot_drop_log() to see why epochs were dropped.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     79\u001b[39m epochs = preprocess_and_epoch(raw, events, event_id)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m X, y = \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m all_X.append(X)\n\u001b[32m     83\u001b[39m all_y.append(y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mextract_features\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_features\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     psd = \u001b[43mepochs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_psd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwelch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     X = np.log(psd.get_data())\n\u001b[32m     60\u001b[39m     X = X.reshape(X.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-256>:10\u001b[39m, in \u001b[36mcompute_psd\u001b[39m\u001b[34m(self, method, fmin, fmax, tmin, tmax, picks, proj, remove_dc, exclude, n_jobs, verbose, **method_kw)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\mne\\epochs.py:2558\u001b[39m, in \u001b[36mBaseEpochs.compute_psd\u001b[39m\u001b[34m(self, method, fmin, fmax, tmin, tmax, picks, proj, remove_dc, exclude, n_jobs, verbose, **method_kw)\u001b[39m\n\u001b[32m   2555\u001b[39m method = _validate_method(method, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m)\n\u001b[32m   2556\u001b[39m \u001b[38;5;28mself\u001b[39m._set_legacy_nfft_default(tmin, tmax, method, method_kw)\n\u001b[32m-> \u001b[39m\u001b[32m2558\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEpochsSpectrum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2559\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2561\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2563\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2564\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_dc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_dc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmethod_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\mne\\time_frequency\\spectrum.py:1438\u001b[39m, in \u001b[36mEpochsSpectrum.__init__\u001b[39m\u001b[34m(self, inst, method, fmin, fmax, tmin, tmax, picks, exclude, proj, remove_dc, n_jobs, verbose, **method_kw)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m   1423\u001b[39m     inst,\n\u001b[32m   1424\u001b[39m     method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1435\u001b[39m     **method_kw,\n\u001b[32m   1436\u001b[39m )\n\u001b[32m   1437\u001b[39m \u001b[38;5;66;03m# get just the data we want\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1438\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minst\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_picks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraise\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[32m   1439\u001b[39m     :, :, \u001b[38;5;28mself\u001b[39m._time_mask\n\u001b[32m   1440\u001b[39m ]\n\u001b[32m   1441\u001b[39m \u001b[38;5;66;03m# compute the spectra\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28mself\u001b[39m._compute_spectra(data, fmin, fmax, n_jobs, method_kw, verbose)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-250>:12\u001b[39m, in \u001b[36m_get_data\u001b[39m\u001b[34m(self, out, picks, item, units, tmin, tmax, copy, on_empty, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\mne\\epochs.py:1633\u001b[39m, in \u001b[36mBaseEpochs._get_data\u001b[39m\u001b[34m(self, out, picks, item, units, tmin, tmax, copy, on_empty, verbose)\u001b[39m\n\u001b[32m   1630\u001b[39m     _validate_type(copy, \u001b[38;5;28mbool\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;66;03m# Handle empty epochs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1633\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_empty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_empty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m_get_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[38;5;66;03m# if called with 'out=False', the call came from 'drop_bad()'\u001b[39;00m\n\u001b[32m   1635\u001b[39m \u001b[38;5;66;03m# if no reasons to drop, just declare epochs as good and return\u001b[39;00m\n\u001b[32m   1636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out:\n\u001b[32m   1637\u001b[39m     \u001b[38;5;66;03m# make sure first and last epoch not out of bounds of raw\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\mne\\epochs.py:1594\u001b[39m, in \u001b[36mBaseEpochs._handle_empty\u001b[39m\u001b[34m(self, on_empty, meth)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.events) == \u001b[32m0\u001b[39m:\n\u001b[32m   1589\u001b[39m     msg = (\n\u001b[32m   1590\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepochs.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt run because this Epochs-object is empty. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1591\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou might want to check Epochs.drop_log or Epochs.plot_drop_log()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1592\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m to see why epochs were dropped.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1593\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[43m_on_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_empty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_klass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;167;43;01mRuntimeError\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\mne\\utils\\check.py:1218\u001b[39m, in \u001b[36m_on_missing\u001b[39m\u001b[34m(on_missing, msg, name, error_klass)\u001b[39m\n\u001b[32m   1216\u001b[39m on_missing = \u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_missing == \u001b[33m\"\u001b[39m\u001b[33mwarning\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m on_missing\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_missing == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_klass(msg)\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m on_missing == \u001b[33m\"\u001b[39m\u001b[33mwarn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1220\u001b[39m     warn(msg)\n",
      "\u001b[31mRuntimeError\u001b[39m: epochs._get_data() can't run because this Epochs-object is empty. You might want to check Epochs.drop_log or Epochs.plot_drop_log() to see why epochs were dropped."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Folder containing all CSVs\n",
    "base_folder = '../data/visual-N170/eegnb_examples/muse2016/'\n",
    "\n",
    "def load_raw_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    sfreq = 256  # typical Muse sampling frequency, adjust if needed\n",
    "    \n",
    "    info = mne.create_info(\n",
    "        ch_names=['TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0'],\n",
    "        sfreq=sfreq,\n",
    "        ch_types=['eeg']*4 + ['misc']*2\n",
    "    )\n",
    "    # Use only EEG channels for raw data\n",
    "    data = df[['TP9', 'AF7', 'AF8', 'TP10', 'Right AUX', 'Marker0']].values.T\n",
    "    \n",
    "    raw = mne.io.RawArray(data, info)\n",
    "    return raw, df\n",
    "\n",
    "def extract_events(df, raw):\n",
    "    markers = df['Marker0'].values\n",
    "    event_samples = np.where(markers != 0)[0]\n",
    "    event_ids = markers[event_samples].astype(int)\n",
    "    events = np.column_stack((event_samples, np.zeros_like(event_samples), event_ids))\n",
    "    \n",
    "    event_id = {str(e): int(e) for e in np.unique(event_ids)}\n",
    "    return events, event_id\n",
    "\n",
    "def preprocess_and_epoch(raw, events, event_id):\n",
    "    # Pick EEG channels only\n",
    "    raw.pick_channels(['TP9', 'AF7', 'AF8', 'TP10'])\n",
    "    \n",
    "    # Filter data\n",
    "    raw.filter(1., 40., fir_design='firwin', verbose=False)\n",
    "    \n",
    "    # Epoching parameters\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events=events,\n",
    "        event_id=event_id,\n",
    "        tmin=-0.2,\n",
    "        tmax=0.8,\n",
    "        baseline=(None, 0),\n",
    "        preload=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    return epochs\n",
    "\n",
    "def extract_features(epochs):\n",
    "    psd = epochs.compute_psd(method='welch', fmin=1, fmax=40, verbose=False)\n",
    "    X = np.log(psd.get_data())\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    y = epochs.events[:, 2]\n",
    "    return X, y\n",
    "\n",
    "# Aggregate all data\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_path = os.path.join(root, file)\n",
    "            print(f\"Processing {csv_path} ...\")\n",
    "            \n",
    "            raw, df = load_raw_from_csv(csv_path)\n",
    "            events, event_id = extract_events(df, raw)\n",
    "            if len(events) == 0:\n",
    "                print(\"No events found, skipping file.\")\n",
    "                continue\n",
    "            epochs = preprocess_and_epoch(raw, events, event_id)\n",
    "            X, y = extract_features(epochs)\n",
    "            \n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "\n",
    "# Combine all data\n",
    "all_X = np.vstack(all_X)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "print(f\"Total epochs: {all_X.shape[0]}, feature dimension: {all_X.shape[1]}\")\n",
    "\n",
    "# Train classifier\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "scores = cross_val_score(clf, all_X, all_y, cv=5)\n",
    "\n",
    "print(f\"Cross-validated accuracy: {scores.mean():.2f} ± {scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dd5a9c04-9734-412d-ab9c-ca36937a22c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 0 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reeya\\AppData\\Local\\Temp\\ipykernel_512\\3866659731.py:7: RuntimeWarning: All epochs were dropped!\n",
      "You might need to alter reject/flat-criteria or drop bad channels to avoid this. You can use Epochs.plot_drop_log() to see which channels are responsible for the dropping of epochs.\n",
      "  epochs = mne.Epochs(\n"
     ]
    }
   ],
   "source": [
    "# Convert event_id keys to strings (values stay int)\n",
    "event_id = {str(int(k)): int(k) for k in np.unique(events[:, 2])}\n",
    "\n",
    "# Pick EEG channels using new syntax\n",
    "raw.pick(['TP9', 'AF7', 'AF8', 'TP10'])\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events,\n",
    "    event_id=event_id,\n",
    "    tmin=-0.2,\n",
    "    tmax=0.8,\n",
    "    baseline=(None, 0),\n",
    "    preload=True,\n",
    "    reject_by_annotation=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"Created {len(epochs)} epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "173fac52-12da-423f-b2d7-fd58e1f5639d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 1\n",
      "Unique event IDs in events: [2]\n",
      "Event IDs in event_id dict: ['2']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of events: {events.shape[0]}\")\n",
    "print(f\"Unique event IDs in events: {np.unique(events[:, 2])}\")\n",
    "print(f\"Event IDs in event_id dict: {list(event_id.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "795191b8-6dcd-4162-8f87-f79b040ae13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_2017-09-13-15.30.01.csv: 197 events\n",
      "data_2017-09-13-15.32.50.csv: 195 events\n",
      "data_2017-09-13-15.35.26.csv: 195 events\n",
      "data_2017-09-13-15.40.17.csv: 194 events\n",
      "data_2017-09-13-15.42.33.csv: 194 events\n",
      "data_2017-09-13-15.45.08.csv: 199 events\n",
      "data_2018-04-15-21.18.48.csv: 197 events\n",
      "data_2018-04-15-21.21.20.csv: 198 events\n",
      "data_2018-05-14-19.14.37.csv: 194 events\n",
      "data_2018-05-14-19.38.10.csv: 196 events\n",
      "data_2018-05-14-19.35.55.csv: 199 events\n",
      "data_2018-05-14-19.38.10.csv: 196 events\n",
      "recording_2018-05-29-20.14.04.csv: 199 events\n",
      "recording_2018-05-31-16.03.39.csv: 7 events\n",
      "recording_2018-06-06-14.34.14.csv: 1 events\n",
      "recording_2018-06-06-15.06.45.csv: 28 events\n",
      "recording_2018-06-06-14.42.27.csv: 1 events\n",
      "recording_2018-06-06-14.45.33.csv: 1 events\n",
      "recording_2018-06-06-18.42.19.csv: 192 events\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            markers = df['Marker0'].values\n",
    "            event_samples = np.where(markers != 0)[0]\n",
    "            print(f\"{file}: {len(event_samples)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "146c936b-5ff5-445c-bf80-e40e87a562b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30720\n",
      "    Range : 0 ... 30719 =      0.000 ...   119.996 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30732\n",
      "    Range : 0 ... 30731 =      0.000 ...   120.043 secs\n",
      "Ready.\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30564\n",
      "    Range : 0 ... 30563 =      0.000 ...   119.387 secs\n",
      "Ready.\n",
      "Skipping recording_2018-05-31-16.03.39.csv (only 7 events)\n",
      "Skipping recording_2018-06-06-14.34.14.csv (only 1 events)\n",
      "Skipping recording_2018-06-06-15.06.45.csv (only 28 events)\n",
      "Skipping recording_2018-06-06-14.42.27.csv (only 1 events)\n",
      "Skipping recording_2018-06-06-14.45.33.csv (only 1 events)\n",
      "Creating RawArray with float64 data, n_channels=6, n_times=30564\n",
      "    Range : 0 ... 30563 =      0.000 ...   119.387 secs\n",
      "Ready.\n",
      "Total epochs collected: 2738\n",
      "Cross-validated accuracy: 0.58 ± 0.07\n"
     ]
    }
   ],
   "source": [
    "min_events = 50\n",
    "\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for root, dirs, files in os.walk(base_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(csv_path)\n",
    "            markers = df['Marker0'].values\n",
    "            event_samples = np.where(markers != 0)[0]\n",
    "\n",
    "            if len(event_samples) < min_events:\n",
    "                print(f\"Skipping {file} (only {len(event_samples)} events)\")\n",
    "                continue\n",
    "\n",
    "            raw, _ = load_raw_from_csv(csv_path)\n",
    "            events, event_id = extract_events(df, raw)\n",
    "            # Make keys string type\n",
    "            event_id = {str(int(k)): int(k) for k in np.unique(events[:, 2])}\n",
    "\n",
    "            epochs = mne.Epochs(\n",
    "                raw.pick(['TP9', 'AF7', 'AF8', 'TP10']),\n",
    "                events,\n",
    "                event_id=event_id,\n",
    "                tmin=-0.2,\n",
    "                tmax=0.8,\n",
    "                baseline=(None, 0),\n",
    "                preload=True,\n",
    "                reject=None,\n",
    "                flat=None,\n",
    "                reject_by_annotation=False,\n",
    "                verbose=False\n",
    "            )\n",
    "            if len(epochs) == 0:\n",
    "                print(f\"No epochs after processing {file}, skipping\")\n",
    "                continue\n",
    "\n",
    "            X, y = extract_features(epochs)\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "\n",
    "all_X = np.vstack(all_X)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "print(f\"Total epochs collected: {all_X.shape[0]}\")\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(kernel='rbf'))\n",
    "scores = cross_val_score(clf, all_X, all_y, cv=5)\n",
    "\n",
    "print(f\"Cross-validated accuracy: {scores.mean():.2f} ± {scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da669f16-a71c-446f-b96e-65fbf91ff238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (191, 16)\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.integrate import simpson\n",
    "\n",
    "def compute_band_power(epochs, bands=None):\n",
    "    if bands is None:\n",
    "        bands = {\n",
    "            'delta': (1, 4),\n",
    "            'theta': (4, 8),\n",
    "            'alpha': (8, 13),\n",
    "            'beta': (13, 30)\n",
    "        }\n",
    "\n",
    "    psds_obj = epochs.compute_psd(\n",
    "        method=\"welch\",  # explicitly use Welch\n",
    "        fmin=1, \n",
    "        fmax=40, \n",
    "        n_fft=256,\n",
    "        verbose=False\n",
    "        )\n",
    "    psds = psds_obj.get_data()  # shape: (n_epochs, n_channels, n_freqs)\n",
    "    freqs = psds_obj.freqs\n",
    "    \n",
    "\n",
    "    psds = 10 * np.log10(psds)  # Convert to dB\n",
    "\n",
    "    band_powers = []\n",
    "    for band, (fmin, fmax) in bands.items():\n",
    "        freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "        power = simpson(psds[:, :, freq_mask], freqs[freq_mask], axis=2)\n",
    "        band_powers.append(power)\n",
    "\n",
    "    features = np.concatenate(band_powers, axis=1)  # shape: (n_epochs, n_channels * bands)\n",
    "    return features\n",
    "\n",
    "# Run this after creating `epochs` for each file\n",
    "X_band = compute_band_power(epochs)\n",
    "y_band = epochs.events[:, 2]\n",
    "\n",
    "print(f\"Feature shape: {X_band.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30872c72-35c9-450a-8130-12cc24b3190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Band Power Classifier Accuracy: 0.63 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf_band = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1))\n",
    "scores_band = cross_val_score(clf_band, X_band, y_band, cv=5)\n",
    "\n",
    "print(f\"Band Power Classifier Accuracy: {scores_band.mean():.2f} ± {scores_band.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dc864a5-6331-4fb7-9892-00020390e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "band_names = ['delta', 'theta', 'alpha', 'beta']\n",
    "channel_names = epochs.info['ch_names']\n",
    "n_bands = len(band_names)\n",
    "n_channels = len(channel_names)\n",
    "\n",
    "X_df = pd.DataFrame(X_band, columns=[f'{ch}_{band}' for band in band_names for ch in channel_names])\n",
    "X_df['Label'] = y_band\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i, band in enumerate(band_names):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    sns.boxplot(data=X_df, x='Label', y=f'AF7_{band}')\n",
    "    plt.title(f'AF7 - {band} band power per class')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2dabd9cd-22cf-4b8b-ae39-fdc0395a64d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 1 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Filter length: 845 samples (3.301 s)\n",
      "\n",
      "Fitting ICA to data using 4 channels (please be patient, this may take a while)\n",
      "Selecting by number: 4 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 0.3s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (4 components)\n",
      "    Zeroing out 1 ICA component\n",
      "    Projecting back using 4 PCA components\n"
     ]
    }
   ],
   "source": [
    "ica = mne.preprocessing.ICA(n_components=4, random_state=42, max_iter='auto')\n",
    "raw.filter(l_freq=1.0, h_freq=None)\n",
    "ica.fit(raw)\n",
    "#ica.plot_components()\n",
    "ica.exclude = [0]  # you can try excluding 0, 1, etc., based on trial\n",
    "raw_clean = ica.apply(raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e3b716c-12bc-4bbc-a126-af5ae9b08573",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_band, y_band, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_band.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y_band)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=8, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4e474ee-2091-4034-b54a-ef3228850130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(clf_band, X_band, y_band, cv=5)\n",
    "cm = confusion_matrix(y_band, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix - SVM Band Power\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d3c7261-9ed7-4a1e-b21e-90b01666ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.13.4 (tags/v3.13.4:8a526ec, Jun  3 2025, 17:46:04) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d86205dd-324c-4015-9eee-ffaa2dffd1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectivity features shape: (191, 6)\n"
     ]
    }
   ],
   "source": [
    "def extract_connectivity_features(epochs):\n",
    "    n_epochs = len(epochs)\n",
    "    n_channels = len(epochs.ch_names)\n",
    "\n",
    "    features = []\n",
    "    for epoch in epochs.get_data():  # shape (n_channels, n_times)\n",
    "        corr = np.corrcoef(epoch)\n",
    "        upper_tri = corr[np.triu_indices(n_channels, k=1)]\n",
    "        features.append(upper_tri)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "X_conn = extract_connectivity_features(epochs)\n",
    "y_conn = epochs.events[:, 2]\n",
    "\n",
    "print(f\"Connectivity features shape: {X_conn.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "58eaa717-814d-4a0b-a02d-7daf9413c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional Connectivity Accuracy: 0.57 ± 0.06\n"
     ]
    }
   ],
   "source": [
    "clf_conn = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1))\n",
    "scores_conn = cross_val_score(clf_conn, X_conn, y_conn, cv=5)\n",
    "\n",
    "print(f\"Functional Connectivity Accuracy: {scores_conn.mean():.2f} ± {scores_conn.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "502ae5ea-fa42-4212-8201-356c65edbc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (4 components)\n",
      "    Zeroing out 1 ICA component\n",
      "    Projecting back using 4 PCA components\n"
     ]
    }
   ],
   "source": [
    "ica.exclude = [0]  # you can try excluding 0, 1, etc., based on trial\n",
    "raw_clean = ica.apply(raw.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66ee736c-5df9-4ffc-afe2-e2fdb4033a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Riemannian classifier accuracy: 0.48 ± 0.02\n"
     ]
    }
   ],
   "source": [
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import MDM\n",
    "\n",
    "covs = Covariances().fit_transform(epochs.get_data())\n",
    "clf_riemann = MDM()\n",
    "scores_riemann = cross_val_score(clf_riemann, covs, y_band, cv=5)\n",
    "\n",
    "print(f\"Riemannian classifier accuracy: {scores_riemann.mean():.2f} ± {scores_riemann.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0a1088c-1992-4c3e-99df-deb3b2b5fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined feature shape: (191, 38)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure everything is numpy arrays and properly shaped\n",
    "X_combined = np.concatenate([X_band, X_conn, covs.reshape(covs.shape[0], -1)], axis=1)\n",
    "X_combined = StandardScaler().fit_transform(X_combined)\n",
    "y_combined = y_band  # All feature sets use same labels\n",
    "\n",
    "print(f\"Combined feature shape: {X_combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3cf50b12-d311-4c5e-b72a-f03d053272c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.6578\n",
      "Epoch 20/50, Loss: 0.6317\n",
      "Epoch 30/50, Loss: 0.6049\n",
      "Epoch 40/50, Loss: 0.5761\n",
      "Epoch 50/50, Loss: 0.5452\n",
      "PyTorch NN Accuracy: 0.13\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_train_tensor = y_train_tensor - 1  # shift labels down by 1\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = EEGNet(input_dim=X_combined.shape[1], hidden_dim=64, output_dim=len(np.unique(y_combined)))\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_tensor).argmax(dim=1).numpy()\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"PyTorch NN Accuracy: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ffde06e-3651-49d7-82a8-d0f6a28a63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.unique(y_train_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f814c0ea-8c1d-42b5-aa49-1d5aa3d70312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  7, 145])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(X_train_tensor)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print(torch.bincount(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3298d880-5af1-48b7-aaea-b0fde44da943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Calculate class weights inversely proportional to their frequency\n",
    "class_counts = torch.bincount(y_train_tensor)\n",
    "class_weights = 1. / class_counts.float()\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "31dcb01d-0b2e-4e82-a6f0-1a378ffc3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Convert tensors to numpy for resampling\n",
    "X_np = X_train_tensor.numpy()\n",
    "y_np = y_train_tensor.numpy()\n",
    "\n",
    "# Separate classes\n",
    "X_class0 = X_np[y_np == 0]\n",
    "X_class1 = X_np[y_np == 1]\n",
    "\n",
    "n_samples = min(len(X_class0), len(X_class1))\n",
    "\n",
    "# Downsample both to same size\n",
    "X_class0_bal = resample(X_class0, n_samples=n_samples, replace=False, random_state=42)\n",
    "X_class1_bal = resample(X_class1, n_samples=n_samples, replace=False, random_state=42)\n",
    "\n",
    "y_class0_bal = np.zeros(n_samples, dtype=np.int64)\n",
    "y_class1_bal = np.ones(n_samples, dtype=np.int64)\n",
    "\n",
    "# Combine balanced data\n",
    "X_bal = np.vstack((X_class0_bal, X_class1_bal))\n",
    "y_bal = np.concatenate((y_class0_bal, y_class1_bal))\n",
    "\n",
    "# Shuffle\n",
    "idx = np.random.permutation(len(y_bal))\n",
    "X_bal = X_bal[idx]\n",
    "y_bal = y_bal[idx]\n",
    "\n",
    "# Convert back to tensors\n",
    "X_train_tensor = torch.tensor(X_bal, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_bal, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "87a4a4ee-076f-44d8-b96a-50a9039ce832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class_counts = torch.bincount(y_train_tensor)\n",
    "class_weights = 1. / class_counts.float()\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f4d13c08-18b1-4d36-b9ca-49ae7fdce98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.6833\n",
      "Epoch 20/50, Loss: 0.6651\n",
      "Epoch 30/50, Loss: 0.6382\n",
      "Epoch 40/50, Loss: 0.6233\n",
      "Epoch 50/50, Loss: 0.5972\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Define model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train_tensor.shape[1], 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(32, 2)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = loss_fn(outputs, y_train_tensor)  # <== use weighted loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/50, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "371ec11b-a1a1-4982-98c4-857cff7064ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced PyTorch NN Accuracy: 0.74\n",
      "Predicted class distribution: tensor([65, 81])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_train_tensor)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    acc = (preds == y_train_tensor).float().mean()\n",
    "    print(f\"Balanced PyTorch NN Accuracy: {acc:.2f}\")\n",
    "    print(\"Predicted class distribution:\", torch.bincount(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c98926e-6778-44e7-92d4-e1992e3074d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_train_tensor, preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Training Data)\")\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "efd829af-e87a-435d-b3d5-5e5ae0a2419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_bal, y_bal, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "66b2c4d6-a8c0-41b4-91cb-844ec74003f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.73\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor)\n",
    "    _, preds_val = torch.max(outputs, 1)\n",
    "    acc = (preds_val == y_val_tensor).float().mean()\n",
    "    print(f\"Validation Accuracy: {acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ba869fd4-90b5-4e2e-9ba8-2770f5233372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if variables exist\n",
    "'y_pred' in globals(), 'y_true' in globals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "77a6f755-c395-4e21-9bd2-d6bee583bf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [2 2 2 2 2 1 2 2 1 1]\n",
      "Ground Truths: [0 0 1 0 0 0 1 1 1 0]\n",
      "Shapes: (191,) (30,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\", y_pred[:10])\n",
    "print(\"Ground Truths:\", y_true[:10])\n",
    "print(\"Shapes:\", y_pred.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd10eff-1d16-4cfa-90df-856700fe50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is a very important observation:\n",
    "\n",
    "❌ Mismatch Detected:\n",
    "y_pred.shape = (191,)\n",
    "\n",
    "y_true.shape = (30,)\n",
    "\n",
    "That means the predictions cover 191 samples, but the current ground truth only covers 30 samples.\n",
    "\n",
    "🔍 What happened?\n",
    "We likely trained on all 191 samples, but only used 30 samples for validation, and then used the model to predict on all 191 instead of just the 30 validation ones.\n",
    "\n",
    "✅ Fix:\n",
    "We need to re-run the model on the validation set only, and compare with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "33220217-b3cd-4e3c-9ca5-4caed2b60ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should already be defined\n",
    "# If not, recreate them from train_test_split\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6bb476f4-3d3b-456d-bec3-34ba519279bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on validation set only\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor)\n",
    "    _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "# Convert to NumPy\n",
    "y_pred = y_pred.numpy()\n",
    "y_true = y_val_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5c50d566-c5a1-46d6-ba44-edadfc532b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0 1 0 0 1 1 1 0 0]\n",
      "Ground Truths: [0 0 1 0 0 0 1 1 1 0]\n",
      "Shapes: (30,) (30,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\", y_pred[:10])\n",
    "print(\"Ground Truths:\", y_true[:10])\n",
    "print(\"Shapes:\", y_pred.shape, y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a81fcd74-ee47-4619-ab49-3adae46ffe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "52c1cbdd-d97e-48fe-ae55-940ebe53c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.62      0.71        16\n",
      "           1       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.73        30\n",
      "   macro avg       0.75      0.74      0.73        30\n",
      "weighted avg       0.76      0.73      0.73        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print precision, recall, F1, support\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e43f89bf-3a02-4db6-9b00-272873153815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9014f73f-cf23-4ddd-90c5-d1b42e741480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability scores (use softmax or sigmoid depending on final layer)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    probs = model(X_val_tensor)\n",
    "    probs = torch.softmax(probs, dim=1)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Convert to NumPy\n",
    "probs = probs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a499e805-bf35-4795-9149-1f8e9248bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "10ef1bec-8696-4bed-accf-c13fcbe553a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.8571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming `probs` are already computed as the probability of class 1:\n",
    "# probs = torch.softmax(model(X_val_tensor), dim=1)[:, 1].numpy()\n",
    "\n",
    "auc_score = roc_auc_score(y_true, probs)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ff62a471-b0d8-4fc6-b6fd-043feb6729e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With an AUC of 0.8571, our model demonstrates strong class separability, which means it is making meaningful predictions and not just guessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b88b26e6-b0d0-46e9-b501-380a59b30ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[10  6]\n",
      " [ 2 12]]\n",
      "True Negatives: 10\n",
      "False Positives: 6\n",
      "False Negatives: 2\n",
      "True Positives: 12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Optional: Access individual values\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Positives: {tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ad079-6bfe-4f4c-a2e8-e130c748853e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435446b1-42ee-4739-9b5b-f83239d2f25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f79fafd8-fa33-4e40-ad1d-ae738716827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ground truth from torch tensor to NumPy\n",
    "y_true = y_val_tensor.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb685f-a154-4b06-af71-63edcdf914ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fa27be25-5e25-4e16-891d-23678b6a7847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1: 89 samples\n",
      "Class 2: 102 samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(y_combined, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"Class {label}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1e645109-b246-4fda-a31c-8053ca9997ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 8.000 (s)\n",
      "[2.81838255 0.4888809  0.44526737 2.44083724]\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppose 'raw' is your MNE Raw object with Muse data\n",
    "# Define channel names\n",
    "ch_names = ['TP9', 'AF7', 'AF8', 'TP10']\n",
    "\n",
    "# Define approximate 3D positions, z=0 for 2D scalp map\n",
    "montage_positions = {\n",
    "    'TP9': [-0.05, -0.05, 0.0],\n",
    "    'AF7': [-0.03,  0.05, 0.0],\n",
    "    'AF8': [ 0.03,  0.05, 0.0],\n",
    "    'TP10':[ 0.05, -0.05, 0.0]\n",
    "}\n",
    "\n",
    "# Create DigMontage object\n",
    "montage = mne.channels.make_dig_montage(ch_pos=montage_positions, coord_frame='head')\n",
    "\n",
    "# Assign montage to your raw object (example: raw)\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# Compute PSD in alpha band 8-12 Hz\n",
    "\n",
    "# Pick channels of interest\n",
    "picks = mne.pick_channels(raw.info[\"ch_names\"], include=ch_names)\n",
    "\n",
    "# Compute PSD with compute_psd()\n",
    "psds = raw.compute_psd(picks=picks, fmin=8, fmax=12).get_data()\n",
    "\n",
    "# Average power per channel\n",
    "band_power = psds.mean(axis=1)\n",
    "\n",
    "print(band_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "03d71589-cc2a-4847-9110-1c56750e064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "# Your channels and their approximate 2D positions on the head (x, y)\n",
    "ch_names = ['TP9', 'AF7', 'AF8', 'TP10']\n",
    "\n",
    "# Define 2D positions (in meters) for the 4 channels (example positions)\n",
    "pos_2d = np.array([\n",
    "    [-0.05, -0.05],  # TP9\n",
    "    [-0.03,  0.05],  # AF7\n",
    "    [ 0.03,  0.05],  # AF8\n",
    "    [ 0.05, -0.05],  # TP10\n",
    "])\n",
    "\n",
    "# Add a tiny jitter (noise) to each coordinate to avoid co-circular error\n",
    "jitter = 1e-5 * np.random.randn(*pos_2d.shape)\n",
    "pos_2d_jiggled = pos_2d + jitter\n",
    "\n",
    "# band_power should be your computed band power array corresponding to ch_names\n",
    "mne.viz.plot_topomap(band_power, pos_2d_jiggled, names=ch_names, show=True)\n",
    "plt.title(\"Alpha Band Power Topomap (jittered positions)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc9286-7cae-48fd-8c92-d2763577c45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b9e8752-06ec-4d38-a61c-fcaf45e7858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "print(mne.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f70996f-ea5a-4b40-8235-4f1d379f2009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af43334d-3587-40c8-8089-7cb11133559e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Pipeline is not fitted yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:53\u001b[39m, in \u001b[36m_raise_or_warn_if_not_fitted\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:1183\u001b[39m, in \u001b[36mPipeline.score\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   1182\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m     Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m score_params = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1072\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[32m   1059\u001b[39m \n\u001b[32m   1060\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1070\u001b[39m \u001b[33;03m    Transformed array.\u001b[39;00m\n\u001b[32m   1071\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1754\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[31mNotFittedError\u001b[39m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNotFittedError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assuming clf is your trained sklearn pipeline (SVM or similar)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m importances = result.importances_mean\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, imp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(importances):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py:286\u001b[39m, in \u001b[36mpermutation_importance\u001b[39m\u001b[34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmax_samples must be <= n_samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    285\u001b[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m baseline_score = \u001b[43m_weights_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m scores = Parallel(n_jobs=n_jobs)(\n\u001b[32m    289\u001b[39m     delayed(_calculate_permutation_scores)(\n\u001b[32m    290\u001b[39m         estimator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X.shape[\u001b[32m1\u001b[39m])\n\u001b[32m    301\u001b[39m )\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\inspection\\_permutation_importance.py:28\u001b[39m, in \u001b[36m_weights_scorer\u001b[39m\u001b[34m(scorer, estimator, X, y, sample_weight)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scorer(estimator, X, y, sample_weight=sample_weight)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:492\u001b[39m, in \u001b[36m_PassthroughScorer.__call__\u001b[39m\u001b[34m(self, estimator, *args, **kwargs)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, estimator, *args, **kwargs):\n\u001b[32m    491\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Method that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:1179\u001b[39m, in \u001b[36mPipeline.score\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   1143\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform the data, and apply `score` with the final estimator.\u001b[39;00m\n\u001b[32m   1144\u001b[39m \n\u001b[32m   1145\u001b[39m \u001b[33;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1176\u001b[39m \u001b[33;03m    Result of calling `score` on the final estimator.\u001b[39;00m\n\u001b[32m   1177\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1178\u001b[39m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _raise_or_warn_if_not_fitted(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1180\u001b[39m     Xt = X\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    160\u001b[39m     value = typ()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:55\u001b[39m, in \u001b[36m_raise_or_warn_if_not_fitted\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[33m\"\u001b[39m\u001b[33mPipeline is not fitted yet.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNotFittedError\u001b[39m: Pipeline is not fitted yet."
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming clf is your trained sklearn pipeline (SVM or similar)\n",
    "result = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "importances = result.importances_mean\n",
    "for i, imp in enumerate(importances):\n",
    "    print(f\"Feature {i}: Importance {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "58d446d8-db0d-4233-aa73-e68df6fe37fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_windows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.windows[idx], \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m train_dataset = EEGDataset(\u001b[43mX_train_windows\u001b[49m, y_train_labels)\n\u001b[32m     29\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m test_dataset = EEGDataset(X_test_windows, y_test_labels)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_windows' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "window_length_samples = 250  # 1 second at 250Hz\n",
    "\n",
    "def segment_data(raw, window_length):\n",
    "    data = raw.get_data(picks=ch_names)  # shape (channels, total_samples)\n",
    "    n_windows = data.shape[1] // window_length\n",
    "    windows = np.array([\n",
    "        data[:, i*window_length:(i+1)*window_length]\n",
    "        for i in range(n_windows)\n",
    "    ])  # shape (n_windows, channels, window_length)\n",
    "    return windows\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, windows, labels):\n",
    "        self.windows = torch.tensor(windows, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.windows[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = EEGDataset(X_train_windows, y_train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = EEGDataset(X_test_windows, y_test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "class SimpleEEGCNN(nn.Module):\n",
    "    def __init__(self, n_channels=4, n_classes=2):\n",
    "        super(SimpleEEGCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_channels, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32 * input_length // 4, 64)  # input_length depends on your window size\n",
    "        self.fc2 = nn.Linear(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = SimpleEEGCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training (one epoch example)\n",
    "model.train()\n",
    "for X_batch, y_batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_batch)\n",
    "    loss = criterion(outputs, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce1b62c7-c4c2-4f6d-adfd-7c34945afe9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m-----------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m labels = y_train[i:i+batch_size]\n\u001b[32m     29\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     32\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mSimpleNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\EEG-ExPy\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2)  # binary classification example\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Suppose X_train, y_train are torch tensors\n",
    "model = SimpleNN(input_dim=X_train.shape[1])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10):  # fewer epochs\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} done, loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a344d7-2efe-4481-acb5-ae9e7e00deca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eegnb-venv)",
   "language": "python",
   "name": "eegnb-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
